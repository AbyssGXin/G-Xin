	$one-hot(独热，只有一个1，其余全为0)$，是一种将原始文本处理成数值型字符的编码转换方式。但有两种弊端：向量维度太高，且丢失了语义信息。后来发明了词向量$(word\ \ embedding)$，在一定程度上解决了$one-hot$的上述两个问题。

​	想要得到词向量，首先要对句子进行分词。分词器的作用不再是仅仅将句子分成单词，更进一步的，他还需要将单词转化成一个唯一的编码，以便下一步在词向量矩阵中查找器对应的词向量。

例句：$Don't\ \  you\ \ love\ \ 🤗\ \ Transformers?\ \ We\ \ sure\ \ do.$

​	一种最简单的分词方式，就是按照空格来对这个句子进行分词，可以得到如下内容：

```
["Don't", "you", "love", "🤗", "Transformers?", "We", "sure", "do."]
```

​	虽然分词成功，但是对于$"Transformers?"$和"do."，标点符号与单词分到一起。如果是这种方式，那么对每一种标点与单词的组合，模型都要学习到一种向量表示，这无疑会增加模型训练的复杂度。因此进行优化，把标点单词单独分出来:

```
["Don", "'", "t", "you", "love", "🤗", "Transformers", "?", "We", "sure", "do", "."]
```

​	尽管$\ ?\ $和$\ .\ $被成功分离出来，但是，缩写单词$\ Don't\ $也被拆分了。我们知道，$\ Don't\ $是$\ do\ not\ $的缩写，因此将其分为下列显然更合理一些。

```
["Do", "n't"]
```

​	但到了这里，事情开始变得复杂起来，不同的模型对于上述情况的处理会采用不同的方式。也就是说，即使是对于同一段文本，由于采用的切分规则不同，我们也可能得到不同的输出。因此，要想正确地使用一个与训练模型，首先需要注意的就是，模型输入所用的切分规则，必须与模型的寻来你数据所采用的切分规则相同。换句话说，一个预训练模型，对应一个与之匹配的tokenizer。

​	$spaCy\ $和$\ Moses\ $是两个比较流行的基于规则的$\ tokenizer\ $，将其应用于上述例句，输出如下：

```
["Do", "n't", "you", "love", "🤗", "Transformers", "?", "We", "sure", "do", "."]
```

​	可以看出，句子被基于空格、标点和缩写词规则进行了切分。这种直观的分词方式非常易于理解，但在面对海量文本词料的场景时，会出现一些问题。在这些场景里，基于空格和标点的切分方式通常会生成非常大的词表(词表指对所有单词和标点去重后得到的几何)。例如，$TransformerXL$使用这种方式进行分词，生成的词表的规模为$267735!$

​	如此规模的词表，将迫使模型维护一个极其庞大的词向量矩阵，这将会导致运算中的时间复杂度和内存占用的上升。实际上，transformers模型的词汇量很少超过50000，尤其是当他们仅在一种语言上进行预训练时。

​	既然这种符合直觉的简单分词方式并不令人满意，那为何不将句子直接拆分为字母(字符级)呢？

​	虽然将句子直接拆分为字母非常简单并且会大大减少内存占用和时间复杂度，但它使模型学习有意义的输入表示变得更加困难。例如，对于字母$\ t\ $，学习他的上下文无关且有意义的向量表示比学习单词$\ today\ $的向量表示要难得多。这就是说，直接字母表示通常会导致性能下降。

​	既然如此，为了两全其美，$transformers$模型使用了词级切分呢和字符级切分的混合，称为子词切分。

## 1. 子词切分

​	子词切分算法依赖这样一种原则：常用词不应该被切分；罕见词应该被切分为更有意义的子词。例如，单词$annoyingly$是一个罕见词，因此，可以被切分为$annoying\ 和\ ly--annoying和ly$更加常见，且$annoyingly$的含义可以由$annoying$和$ly$组合生成。

​	子词切分允许模型保持一个规模相对合理的词汇量，同时能够学习有意义的上下文无关表示。此外，子词切分还可以是模型能够处理他以前从未见过的词，将他们分解为已知的子词。例如，使用$BertTokenizer$来切分句子$I\ \ have\ \ a\ \ new\ \ GPU!$，其结果如下：

```
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
tokenizer.tokenize("I have a new GPU!")

["i", "have", "a", "new", "gp", "##u", "!"]
```

​	由于使用了不区分大小写的模型，因此，句子中的字母都被处理成了小写。从结果中可以看出，$["i",\ "have",\ "a",\ "new"]$出现在$tokenizer$的词表中，但是$gpu$并不在词表中。因此，$tokenizer$将$gpu$切分成了两个存在于词表中的子词：$["gp",\ "\#\#u"]$。$\#\#$表示改$token$的剩余部分应该直接连接到上一个$token$上(这将会被用于解码层)。

​	另外一个例子，我们使用$XLNetTokenizer$：

```
from transformers import XLNetTokenizer
tokenizer = XLNetTokenizer.from_pretrained("xlnet-base-cased")
tokenizer.tokenize("Don't you love 🤗 Transformers? We sure do.")

["_Don", "'", "t", "_you", "_love", "_", "🤗", "_", "Transform", "ers", "?", "_We", "_sure", "_do", "."]
```

​	有关其中的$\ \_\ $的含义，在下面讲$SentencePiece$时再进行说明。如你所见，不常见的单词$Transformers$已经被切分为两个更常见的子词$Transform$和$ers$。

### 1.1 字节对编码$(Byte-Pair\ \ Encoding, BPE)$

​	$BPE$首次在论文[Neural Machine Translation of Rare Words with Subword Units(Sennrich et al., 2015)](https://arxiv.org/abs/1508.07909)中被提出。$BPE$首先需要依赖一个可以预先讲训练数据切分成单词的$tokenizer$，他们可以一些简单的基于空格的$tokenizer$，如$GPT-2，Roberta$等；也可以是一些更加复杂的、增加了一些规则的$tokenizer$，如$XLM、FlauBERT$。

​	在使用了这些$tokenizer$后，我们可以得到一个在训练数据中出现过的单词的集合以及他们对应的频数。下一步，$BPE$使用这个集合中的所有符号(将单词拆分为字母)创建一个基本词表，然后学习合并规则以将基本词表的两个符号形成一个新符号 ，从而实现对基本词表的更新。他将持续这一操作，直到词表的大小达到了预置的规模。值得注意的是，这个预置的词表大小是一个超参数，需要提前指定。

​	举个例子，假设经过预先切分后，单词及对应的频数如下：

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

​	因此，基本词表的内容为$["b", "g", "h", "n", "p", "s", "u"]$。对应的，将所有的单词按照基本词表中的字母拆分，得到：

```
("h" "u" "g", 10),("p" "u" "g", 5),("p" "u" "n", 12),("b" "u" "n", 4),("h" "u" "g" "s", 5)
```

​	接下来，$BPE$计算任意两个字母(符号)拼接到一起时，出现在语料中的频数，然后选择频数最大的字母(符号)对。接上例，$hu\ $组合的频数为15($hug$出现了10次，$hugs$中出现了5次)。在上面的例子中，频数最高的符号对是$ug$，一种有20次。因此，$tokenizer$学习到的第一个合并规则就是将所有的$ug$合并到一起。于是，基本词表变为：

```
("h" "ug", 10),("p" "ug", 5),("p" "u" "n", 12),("b" "u" "n", 4),("h" "ug" "s", 5)
```

​	应用相同的算法，下一个频数最高的组合是$un$，出现了16次，于是$un$被添加到词表中；接下来是$hug$，即$h$与第一步得到的$ug$组合的频数最高，共有15次，于是$hug$被添加到了词表中。

​	此时，词表的内容为$["b", "g","h", "n", "p", "s", "u", "ug", "un", "hug"]$，原始的单词按照词表拆分后的内容如下：

```
("hug", 10),("p" "ug", 5),("p" "un", 12),("b" "un", 4),("hug" "s",5)
```

​	假定$BPE$的训练到这一步就停止，接下来就是利用它学习到的这些规则来切分新的单词(只要新单词中没有超出基本词表之外的符号)。例如，单词$bug$将会被切分为$["b","ug"]$，但是单词$mug$将会被切分为$["", "ug"]$——这是因为$m$不在词表中。

​	正如之前提到的，词表的规模——也就是基本词表的大小加上合并后的单词的数量——是一个超参数。例如，对于$GPT$而言，其词表的大小为40478，其中，基本字符一共有478个，合并后的词有40000个。

#### 1.1.1 字节级的$BPE$

​	一个包含所有基本字符的词表也可以非常的大，例如，如果将所有的$unicode$字符视为基本字符。针对这一问题，$GPT-2$使用字节来作为基本词表，这种方法保证了所有的基本字符都包含在了词表中，且基本词表的大小被限定为256.再加上一些用于处理标点的规则，$GPT2$的$tokenizer$可以切分任何英文句子，而不必引入符号。$GPT-2$的词表大小为50257，包含了256个字节级的基本词表，一个用于标识句子结尾的特殊字符，以及学习到的50000个合并单词。

### 1.2 $WordPiece$

​	$WordPiece$是一种被应用于$BERT$，$DistilBERT$和$Electra$的子词切分算法，它与$BPE$算法非常像。首先，他初始化一个包含所有出现在训练数据中的字符的词表，然后递归地学习一些合并规则。与$BPE$不同的是，$WordPiece$并不选择出现的频数最高的组合，而是选择可以最大化训练数据可能性的组合。

​	仍然拿上面的例子来说明。最大化训练数据的可能性就等同于找到这样的符号对，他的概率除以其第一个符号的概率与其第二个符号的概率的乘积，所得的值是所有可能的符号对中最大的。例如，如果$ug$的概率除以$u$的概率再除以$g$的概率得到的结果比其他任何一种组合的结果都大，那么就将$ug$组合起来。从直觉上说，$WordPiece$在合并两个符号前，评估了一下这种合并所带来的损失(合并后的概率除以合并前的概率)，以确保这种合并是有价值的。

### 1.3 $Unigram$

​	论文[Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://arxiv.org/pdf/1804.10959.pdf)提出了基于子词的$Unigram$的切分方法。与$BPE$和$WordPiece$不同的是，$Unigram$以一个大规模的符号库作为其初始词表，然后采用迭代的方式来不断降低词表的规模。例如，初始词表可以是一个包含所有的预先切分的单词和所有常见子词的集合。

​	在每一个训练步中，$Unigram$算法在给定当前词表和一元语言模型的情况下定义了训练模型的损失函数(通常为对数似然函数)。然后，对于词表中的每一个符号，算法都会计算出如果将该符号从词表中移除，全局损失会增加多少。随后，$Unigram$会移除一定百分比(通常是10%或20%)的那些使损失增加最少的符号。

​	重复上述训练过程，直到词表达到预定的规模。由于$Unigram$总是保留基本字符，因此它可以切分任何单词。

​	由于$Unigram$并不像$BPE$和$WordPiece$那样依赖合并规则，所以，经过训练的$Unigram$在切分文本时可能会有多种选择。假设一个$Unigram\ \ tokenizer$的词表如下：

```
["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
```

​	单词$hugs$可以被切分为$["hug",\ "s"]、["h","ug", "s"]或["h", "u", "g","s]$。那么，究竟该选择哪一种呢？$Unigram$在保存词表的基础上，也保存了训练语料库中每个$token$的概率，以便在训练后计算每一种可能的切分的概率。$Unigram$在实践中只是简单地选择概率最后的切分作为最终结果，但也提供了根据概率对可能的切分进行采样的能力。

### 1.4 $SentencePiece$

​	上文介绍了所有的切分算法都有一个共同的问题：它们都假定输入的文本使用空格进行单词划分。然而，并不是所有的语言都是这样的。针对这一点，一种解决方案是，使用基于特定语言的$tokenizer$进行预先分词，例如，$XLM$使用了针对中文、日文和泰文的$tokenizer$。一个更一般的解决方案，在论文[SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing(Kudo et al., 2018)](https://arxiv.org/pdf/1808.06226.pdf)中被提出。这种方法将输入的文本视为元输入流，进而将空格包含在要使用的字符集中。然后，它再使用$BPE$或$Unigram$算法来构建一个合适的词表。

​	$XLNetTokenizer$使用了$SentencePiece$，这也就是为什么在上面的例子中，$\ \_\ $出现在其切分结果中。基于$SentencePiece$的解码非常容易：将各个$token$拼接起来，把$\ \_\ $替换为空格即可。

















